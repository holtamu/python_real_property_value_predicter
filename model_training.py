import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. ë°ì´í„° ë¡œë“œ
try:
    df = pd.read_csv("geumcheon_apt_2024_cleaned.csv")
    print("âœ… ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except FileNotFoundError:
    print("âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. main.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    exit()

# 2. [ìˆ˜ì •] íŠ¹ì§•(X)ê³¼ ì •ë‹µ(y) ë¶„ë¦¬
# ë²•ì •ë™(ê¸€ì)ì„ 0ê³¼ 1ë¡œ ë³€í™˜í•˜ëŠ” ì›-í•« ì¸ì½”ë”©ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
df_encoded = pd.get_dummies(df, columns=['ë²•ì •ë™'])

# ì…ë ¥ ë°ì´í„°: ì „ìš©ë©´ì , ì•„íŒŒíŠ¸ë‚˜ì´ + ë²•ì •ë™_ê°€ì‚°ë™, ë²•ì •ë™_ë…ì‚°ë™, ë²•ì •ë™_ì‹œí¥ë™ ë“±
# ì¶œë ¥ ë°ì´í„°: ê±°ë˜ê¸ˆì•¡
# filterë¥¼ ì¨ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ Xë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
X = df_encoded.filter(regex='ì „ìš©ë©´ì |ì•„íŒŒíŠ¸ë‚˜ì´|ë²•ì •ë™_').values
y = df_encoded['ê±°ë˜ê¸ˆì•¡'].values

# 3. ë°ì´í„°ì…‹ ë¶„í•  (í•™ìŠµìš© 80%, í…ŒìŠ¤íŠ¸ìš© 20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. [ìˆ˜ì •] ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ ì„¤ê³„
# input_shapeë¥¼ ê³ ì •ëœ (2,)ê°€ ì•„ë‹ˆë¼ Xì˜ ì»¬ëŸ¼ ê°œìˆ˜ì— ë§ì¶° ìë™ìœ¼ë¡œ ì„¤ì •í•˜ê²Œ ë°”ê¿¨ìŠµë‹ˆë‹¤.
model = tf.keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)), 
    layers.Dense(32, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)
])

# 6. ëª¨ë¸ ì„¤ì • (ì»´íŒŒì¼)
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# 7. ì¸ê³µì§€ëŠ¥ í•™ìŠµ ì‹œì‘
print("\nğŸ¤– ë²•ì •ë™ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì¸ê³µì§€ëŠ¥ í•™ìŠµ ì¤‘...")
history = model.fit(
    X_train_scaled, y_train, 
    epochs=100, 
    batch_size=32, 
    validation_split=0.2,
    verbose=0 
)
print("âœ¨ í•™ìŠµ ì™„ë£Œ!")

# 8. í•™ìŠµ ê²°ê³¼ ì‹œê°í™” (Loss ê·¸ë˜í”„)
# --- ìœˆë„ìš° í•œê¸€ í°íŠ¸ ì„¤ì • ì¶”ê°€ ---
plt.rc('font', family='Malgun Gothic') # ë§‘ì€ ê³ ë”• ì„¤ì •
plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
# ---------------------------------
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='í•™ìŠµ ì†ì‹¤ (Train Loss)')
plt.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤ (Val Loss)')
plt.title('ëª¨ë¸ í•™ìŠµ ê³¼ì • (Loss)')
plt.xlabel('ë°˜ë³µ íšŸìˆ˜ (Epochs)')
plt.ylabel('ì†ì‹¤ ê°’ (MSE)')
plt.legend()
plt.show()

# 9. ëª¨ë¸ í‰ê°€
loss, mae = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"\nğŸ“Š ëª¨ë¸ í‰ê°€ ê²°ê³¼ (MAE): í‰ê·  ì•½ {round(mae, 2)}ë§Œì› ì •ë„ì˜ ì˜¤ì°¨ê°€ ë°œìƒí•©ë‹ˆë‹¤.")

# 10. [ìˆ˜ì •] ì‹¤ì œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
# ì˜ˆ: ì „ìš©ë©´ì  84ã¡, ì•„íŒŒíŠ¸ ë‚˜ì´ 10ë…„, ì‹œí¥ë™(ì„¸ ë²ˆì§¸ ë™ë„¤ë¼ê³  ê°€ì •) ì•„íŒŒíŠ¸ì˜ ì˜ˆìƒ ê°€ê²©ì€?
# ì›-í•« ì¸ì½”ë”© ìˆœì„œì— ë§ì¶°ì„œ ë°ì´í„°ë¥¼ ë„£ì–´ì¤˜ì•¼ í•©ë‹ˆë‹¤.
# [ë©´ì , ë‚˜ì´, ë²•ì •ë™_ê°€ì‚°ë™(0), ë²•ì •ë™_ë…ì‚°ë™(0), ë²•ì •ë™_ì‹œí¥ë™(1)] í˜•íƒœ ì˜ˆì‹œ:
# *ì£¼ì˜: ì‹¤ì œ ë°ì´í„°ì˜ ë™ë„¤ ìˆœì„œì— ë”°ë¼ 1ì˜ ìœ„ì¹˜ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
test_columns = df_encoded.filter(regex='ì „ìš©ë©´ì |ì•„íŒŒíŠ¸ë‚˜ì´|ë²•ì •ë™_').columns
print(f"\nì…ë ¥ ìˆœì„œ í™•ì¸: {list(test_columns)}")

# ì‹œí¥ë™(ë²•ì •ë™_ì‹œí¥ë™=1)ì„ ê°€ì •í•˜ê³  í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
# ë³´í†µ ê°€ì‚°, ë…ì‚°, ì‹œí¥ ìˆœì´ë¯€ë¡œ 0, 0, 1ë¡œ ë„£ì–´ë´…ë‹ˆë‹¤.
sample_data = np.array([[84, 10, 0, 0, 1]]) 
sample_scaled = scaler.transform(sample_data)
prediction = model.predict(sample_scaled)

print(f"ğŸ  ì˜ˆì¸¡ ê²°ê³¼: {list(test_columns)} ì¡°ê±´ì˜ ì˜ˆìƒê°€ëŠ” ì•½ {round(prediction[0][0], 2)}ë§Œì›ì…ë‹ˆë‹¤.")